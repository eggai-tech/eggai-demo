# =============================================================================
# EggAI Demo - Default Configuration
# =============================================================================
# Sensible defaults that work out of the box with LM Studio.
# Override any value by setting the environment variable or in .env
# =============================================================================

# -----------------------------------------------------------------------------
# Infrastructure (Docker services)
# -----------------------------------------------------------------------------
KAFKA_BOOTSTRAP_SERVERS=localhost:19092
VESPA_URL=http://localhost:8080
TEMPORAL_SERVER_URL=localhost:7233
MLFLOW_TRACKING_URI=http://localhost:5001
OTEL_ENDPOINT=http://localhost:4318

# -----------------------------------------------------------------------------
# Language Model - Default: LM Studio (local, no API key required)
# -----------------------------------------------------------------------------
# To use OpenAI instead, override in .env:
#   OPENAI_API_KEY=sk-your-key
#   And set each agent's model to openai/gpt-4o-mini

# Shared LM Studio settings
LM_STUDIO_API_BASE=http://localhost:1234/v1/
LM_STUDIO_API_KEY=lm-studio

# Default model for all agents (can be overridden per-agent)
DEFAULT_LANGUAGE_MODEL=lm_studio/gemma-3-12b-it-qat
DEFAULT_LANGUAGE_MODEL_API_BASE=http://localhost:1234/v1/

# -----------------------------------------------------------------------------
# Per-Agent Language Model Settings
# -----------------------------------------------------------------------------
# Each agent can use a different model. Defaults inherit from above.

# Triage Agent
TRIAGE_LANGUAGE_MODEL_API_BASE=http://localhost:1234/v1/
TRIAGE_LANGUAGE_MODEL=lm_studio/gemma-3-12b-it-qat

# Billing Agent
BILLING_LANGUAGE_MODEL_API_BASE=http://localhost:1234/v1/
BILLING_LANGUAGE_MODEL=lm_studio/gemma-3-12b-it-qat

# Claims Agent
CLAIMS_LANGUAGE_MODEL_API_BASE=http://localhost:1234/v1/
CLAIMS_LANGUAGE_MODEL=lm_studio/gemma-3-12b-it-qat

# Policies Agent
POLICIES_LANGUAGE_MODEL_API_BASE=http://localhost:1234/v1/
POLICIES_LANGUAGE_MODEL=lm_studio/gemma-3-12b-it-qat

# Escalation Agent
ESCALATION_LANGUAGE_MODEL_API_BASE=http://localhost:1234/v1/
ESCALATION_LANGUAGE_MODEL=lm_studio/gemma-3-12b-it-qat

# -----------------------------------------------------------------------------
# Triage Classifier Configuration
# -----------------------------------------------------------------------------
# Available versions: v0, v1, v2, v3, v4, v5, v6, v7
# - v0-v4: DSPy-based (zero-shot, few-shot, optimized prompts)
# - v5: PyTorch attention network
# - v6: OpenAI fine-tuned model
# - v7: Gemma fine-tuned via HuggingFace

TRIAGE_CLASSIFIER_VERSION=v4

# Classifier v3 (few-shot) settings
TRIAGE_CLASSIFIER_V3_MODEL_NAME=fewshot_classifier_n_all
TRIAGE_CLASSIFIER_V3_MODEL_VERSION=1

# COPRO optimizer settings (v2, v4)
TRIAGE_COPRO_DATASET_SIZE=30
TRIAGE_COPRO_BREADTH=5
TRIAGE_COPRO_DEPTH=2
TRIAGE_TEST_DATASET_SIZE=50

# -----------------------------------------------------------------------------
# Agent Ports
# -----------------------------------------------------------------------------
FRONTEND_PORT=8000
FRONTEND_METRICS_PORT=9097
TRIAGE_METRICS_PORT=9096
BILLING_METRICS_PORT=9091
CLAIMS_METRICS_PORT=9092
POLICIES_METRICS_PORT=9093
ESCALATION_METRICS_PORT=9094
AUDIT_METRICS_PORT=9095

# -----------------------------------------------------------------------------
# MLflow & Model Storage
# -----------------------------------------------------------------------------
AWS_ACCESS_KEY_ID=user
AWS_SECRET_ACCESS_KEY=password
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000

# -----------------------------------------------------------------------------
# MinIO (S3-compatible storage for document ingestion)
# -----------------------------------------------------------------------------
MINIO_ENDPOINT_URL=http://localhost:9000
MINIO_ACCESS_KEY=user
MINIO_SECRET_KEY=password
MINIO_POLL_INTERVAL=30

# -----------------------------------------------------------------------------
# Feature Flags
# -----------------------------------------------------------------------------
TRACING_ENABLED=true
CACHE_ENABLED=false

# -----------------------------------------------------------------------------
# Deployment Namespace (optional)
# -----------------------------------------------------------------------------
# Use this to run multiple isolated deployments on the same infrastructure
# Example values: pr-123, staging, prod
# This will prefix Kafka topics, Temporal namespaces, and Vespa app names
# DEPLOYMENT_NAMESPACE=

# -----------------------------------------------------------------------------
# Optional: OpenAI API Key (for v6 classifier or cloud models)
# -----------------------------------------------------------------------------
# OPENAI_API_KEY=sk-your-key

# -----------------------------------------------------------------------------
# Optional: Guardrails Token (for content moderation)
# -----------------------------------------------------------------------------
# GUARDRAILS_TOKEN=your-guardrails-token
